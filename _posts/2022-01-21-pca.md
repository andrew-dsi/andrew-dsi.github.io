---
layout: post
title: Compressing Feature Space For Classification Using PCA
image: "/posts/pca-title-img.png"
tags: [PCA, Machine Learning, Python]
---

In this project we use Principal Component Analysis (PCA) to compress 100 unlabelled, sparse features into a more manageable number for classiying buyers of Ed Sheeran's latest album.

# Table of contents

- [00. Project Overview](#overview-main)
    - [Context](#overview-context)
    - [Actions](#overview-actions)
    - [Results](#overview-results)
    - [Growth/Next Steps](#overview-growth)
- [01. Data Overview](#data-overview)
- [02. PCA Overview](#pca-overview)
- [03. Data Preparation](#pca-data-prep)
- [04. Fitting PCA](#pca-fit)
- [05. Analysis Of Explained Variance](#pca-variance)
- [06. Applying PCA](#pca-application)
- [07. Classification Model](#pca-classification)
- [08. Growth & Next Steps](#growth-next-steps)

___

# Project Overview  <a name="overview-main"></a>

### Context <a name="overview-context"></a>

Our client is looking to promote Ed Sheeran's new album - and want to be both targeted with their customer communications, and as efficient as possible with their marketing budget.

As a proof-of-concept they would like us to build a classification model for customers who purchased Ed's *last* album based upon a small sample of listening data they have acquired for some of their customers at that time.

If we can do this successfully, they will look to purchase up-to-date listening data, apply the model, and use the predicted probabilities to promote to customers who are most likely to purchase.

The sample data is short but wide.  It contains only 356 customers, but for each, columns that represent the percentage of historical listening time allocated to each of 100 artists.  On top of these, the 100 columns do not contain the artist in question, instead being labelled *artist_01, artist_02* etc.

We will need to compress this data into something more manageable for classification!

<br>
<br>
### Actions <a name="overview-actions"></a>

We firstly needed to bring in the required data, both the historical listening sample, and the flag showing which customers purchased Ed Sheeran's last album.  We ensure we  split our data a training set & a test set, for classification purposes.  For PCA, we ensure that we scale the data so that all features exist on the same scale.

We then apply PCA without any specified number of components - which allows us to examine & plot the percentage of explained variance for every number of components.  Based upon this we make a call to limit our dataset to the number of components that make up 75% of the variance of the initial feature set (rather than limiting to a specific number of components).  We apply this rule to both our training set (using fit_transform) and our test set (using transform only)

With this new, compressed dataset, we apply a Random Forest Classifier to predict the sales of the album, and we assess the predictive performance!

<br>
<br>

### Results <a name="overview-results"></a>

Through enforcing a limitation on our data, to contain components that make up 75% of the variance of the initial feature set, we are left with XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX new features.

Using these features, our Random Forest Classifier was able to predict customers that would purchase Ed Sheeran's last album with a Classification Accuracy of XXXX %
XXXX
XXXX
XXXX
XXXX

<br>
<br>
### Growth/Next Steps <a name="overview-growth"></a>

XXXX
XXXX

Use initial notes from modelling projects - other types of models, trialling a search for optimal number of components specifically for classification accuracy

More hyperparameter tuning on RF
XXX
XXXX
XXX

<br>
<br>

___

# Data Overview  <a name="data-overview"></a>

Our dataset contains only 356 customers, but 102 columns.

In the code below, we:

* Import the required python packages & libraries
* Import the data from the database
* Drop the ID column for each customer
* Shuffle the dataset
* Analyse the class balance between album buyers, and non album buyers

<br>
```python

# import required Python packages
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.ensemble import RandomForestClassifier
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

# import data
data_for_model = ...

# drop the id column
data_for_model.drop("user_id", axis = 1, inplace = True)

# shuffle the data
data_for_model = shuffle(data_for_model, random_state = 42)

# analyse the class balance
data_for_model["purchased_album"].value_counts(normalize = True)

```
<br>

From the last step in the above code, we see that 53% of customers in our sample did purchase Ed's last album, and 47% did not. Since this is evenly balanced, we can most likely rely solely on *Classification Accuracy* when assessing the performance of the classification model later on.

After these steps, we have a dataset that looks like the below sample (not all columns shown):
<br>
<br>

| **purchased_album** | **artist1** | **artist2** | **artist3** | **artist4** | **artist5** | **artist6** | **artist7** | **…** |
|---|---|---|---|---|---|---|---|---|
| 1 | 0.0278 | 0 | 0 | 0 | 0 | 0.0036 | 0.0002 | … |
| 1 | 0 | 0 | 0.0367 | 0.0053 | 0 | 0 | 0.0367 | … |
| 1 | 0.0184 | 0 | 0 | 0 | 0 | 0 | 0 | … |
| 0 | 0.0017 | 0.0226 | 0 | 0 | 0 | 0 | 0 | … |
| 1 | 0.0002 | 0 | 0 | 0 | 0 | 0 | 0 | … |
| 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | … |
| 1 | 0.0042 | 0 | 0 | 0 | 0 | 0 | 0 | … |
| 0 | 0 | 0 | 0.0002 | 0 | 0 | 0 | 0 | … |
| 1 | 0 | 0 | 0 | 0 | 0.1759 | 0 | 0 | … |
| 1 | 0.0001 | 0 | 0.0001 | 0 | 0 | 0 | 0 | … |
| 1 | 0 | 0 | 0 | 0.0555 | 0 | 0.0003 | 0 | … |
| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | … |
| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | … |

<br>
The data is at customer level.  We have a binary column showing whether the customer purchased the prior album or not, and following that 100 columns containing the percentage of historical listening time allocated to each artist.  We do not know the names of these artists.

From the above sample, we can also see the sparsity of the data, customers do not listen to all artists and therefore many of the values are 0.

<br>
# PCA Overview  <a name="data-overview"></a>

Principal Component Analysis (PCA) is often used as a *Dimensionality Reduction* technique that can reduce a large set of variables down to a smaller set, that still contains most of the original information.

In other words, PCA takes a high number of dimensions, or variables and boils them down into a much smaller number of new variables - each of which is called a *principal component*.  These new *components* are somewhat abstract - they are a blend of some of the original features where the PCA algorithm found they were correlated.  By blending the original variables rather than just removing them, the hope is that we still keep much of the key information that was held in the original feature set.

Dimensionality Reduction techniques like PCA are mainly used to simplify the space in which we're operating.  Attempting to apply the k-means clustering algorithm (for example) across hundreds or thousands of features can be computationally expensive, PCA reduces this vastly while maintaining much of the key information contained in the data.  But PCA doesn’t have applications just within the realms of unsupervised learning, it could just as easily be applied to a set of input variables in a supervised learning approach - exactly like we will do here!

In supervised learning, we often focus on *Feature Selection* where we look to remove variables that are not deemed to be important in predicting our output.  PCA is often used in a similar way, although in this case we aren't explicitly *removing* variables - we are simply creating a smaller number of *new* ones that contain much of the information contained in the original set.

<br>
# Data Preparation  <a name="pca-data-prep"></a>

<br>
##### Split Out Data For Modelling

In the next code block we do two things, we firstly split our data into an X object which contains only the predictor variables, and a y object that contains only our dependent variable.

Once we have done this, we split our data into training and test sets to ensure we can fairly validate the accuracy of the predictions on data that was not used in training. In this case, we have allocated 80% of the data for training, and the remaining 20% for validation. We make sure to add in the stratify parameter to ensure that both our training and test sets have the same proportion of customers who did, and did not, sign up for the delivery club - meaning we can be more confident in our assessment of predictive performance.

```python

# split data into X and y objects for modelling
X = data_for_model.drop(["purchased_album"], axis = 1)
y = data_for_model["purchased_album"]

# split out training & test sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42, stratify = y)

```

<br>
##### Feature Scaling

Feature Scaling is extremely important when applying PCA - it means that the algorithm can successfully "judge" the correlations between the variables and effectively create the principal compenents for us.  The general consensus is to apply Standardisation rather than Normalisation.

The below code uses the in-built StandardScaler functionality from scikit-learn to apply Standardisation to all of our variables.  The reason we create a new object (here called data_for_clustering_scaled) is that we want to use the scaled data for clustering, but when profiling the clusters later on, we may want to use the actual percentages as this may make more intuitive business sense, so it's good to have both options available!

```python

# create our scaler object
scale_norm = MinMaxScaler()

# normalise the data
data_for_clustering_scaled = pd.DataFrame(scale_norm.fit_transform(data_for_clustering), columns = data_for_clustering.columns)

```

<br>
### Finding A Good Value For k <a name="kmeans-k-value"></a>

At this point here, our data is ready to be fed into the k-means clustering algorithm.  Before that however, we want to understand what number of clusters we want the data split into.

In the world of unsupervised learning, there is no *right or wrong* value for this - it really depends on the data you are dealing with, as well as the unique scenario you're utilising the algorithm for.  From our client, having a very high number of clusters might not be appropriate as it would be too hard for the business to understand the nuance of each in a way where they can apply the right strategies.

Finding the "right" value for k, can feel more like art than science, but there are some data driven approaches that can help us!  

The approach we will utilise here is known as *Within Cluster Sum of Squares (WCSS)* which measures the sum of the squared euclidean distances that data points lie from their closest centroid.  WCSS can help us understand the point where adding *more clusters* provides little extra benefit in terms of separating our data.

By default, the k-means algorithm within scikit-learn will use k = 8 meaning that it will look to split the data into eight distinct clusters.  We want to find a better value that fits our data, and our task!

In the code below we will test multiple values for k, and plot how this WCSS metric changes.  As we increase the value for k (in other words, as we increase the number or centroids or clusters) the WCSS value will always decrease.  However, these decreases will get smaller and smaller each time we add another centroid and we are looking for a point where this decrease is quite prominent *before* this point of diminishing returns.

```python

# set up range for search, and empty list to append wcss scores to
k_values = list(range(1,10))
wcss_list = []

# loop through each possible value of k, fit to the data, append the wcss score
for k in k_values:
    kmeans = KMeans(n_clusters = k, random_state = 42)
    kmeans.fit(data_for_clustering_scaled)
    wcss_list.append(kmeans.inertia_)

# plot wcss by k
plt.plot(k_values, wcss_list)
plt.title("Within Cluster Sum of Squares -  by k")
plt.xlabel("k")
plt.ylabel("WCSS Score")
plt.tight_layout()
plt.show()

```
<br>
That code gives us the below plot - which visualises our results!

<br>
![alt text](/img/posts/kmeans-optimal-k-value-plot.png "K-Means Optimal k Value Plot")

<br>
Based upon the shape of the above plot - there does appear to be an elbow at k = 3.  Prior to that we see a significant drop in the WCSS score, but following the decreases are much smaller, meaning this could be a point that suggests adding *more clusters* will provide little extra benefit in terms of separating our data.  A small number of clusters can be beneficial when considering how easy it is for the business to focus on, and understand, each - so we will continue on, and fit our k-means clustering solution with k = 3.

<br>
### Model Fitting <a name="kmeans-model-fitting"></a>

The below code will instantiate our k-means object using a value for k equal to 3.  We then fit this object to our scaled dataset to separate our data into three distinct segments or clusters.

```python

# instantiate our k-means object
kmeans = KMeans(n_clusters = 3, random_state = 42)

# fit to our data
kmeans.fit(data_for_clustering_scaled)

```

<br>
### Append Clusters To Customers <a name="kmeans-append-clusters"></a>

With the k-means algorithm fitted to our data, we can now append those clusters to our original dataset, meaning that each customer will be tagged with the cluster number that they most closely fit into based upon their sales data over each product area.

In the code below we tag this cluster number onto our original dataframe.

```python

# add cluster labels to our original data
data_for_clustering["cluster"] = kmeans.labels_

```

<br>
### Cluster Profiling <a name="kmeans-cluster-profiling"></a>

Once we have our data separated into distinct clusters, our client needs to understand *what is is* that is driving the separation.  This means the business can understand the customers within each, and the behaviours that make them unique.

<br>
##### Cluster Sizes

In the below code we firstly assess the number of customers that fall into each cluster.

<br>
```python

# check cluster sizes
data_for_clustering["cluster"].value_counts(normalize=True)

```
<br>

Running that code shows us that the three clusters are different in size, with the following proportions:

* Cluster 0: **73.6%** of customers
* Cluster 2: **14.6%** of customers
* Cluster 1: **11.8%** of customers

Based on these results, it does appear we do have a skew toward Cluster 0 with Cluster 1 & Cluster 2 being proportionally smaller.  This isn't right or wrong, it is simply showing up pockets of the customer base that are exhibiting different behaviours - and this is *exactly* what we want.

<br>
##### Cluster Attributes

To understand what these different behaviours or characteristics are, we can look to analyse the attributes of each cluster, in terms of the variables we fed into the k-means algorithm.

<br>
```python

# profile clusters (mean % sales for each product area)
cluster_summary = data_for_clustering.groupby("cluster")[["Dairy","Fruit","Meat","Vegetables"]].mean().reset_index()

```
<br>
That code results in the following table...

| **Cluster** | **Dairy** | **Fruit** | **Meat** | **Vegetables** |
|---|---|---|---|---|
| 0 | 22.1% | 26.5% | 37.7% | 13.8%  |
| 1 | 0.2% | 63.8% | 0.4% | 35.6%  |
| 2 | 36.4% | 39.4% | 2.9% | 21.3%  |

<br>
For *Cluster 0* we see a reasonably significant portion of spend being allocated to each of the product areas.  For *Cluster 1* we see quite high proportions of spend being allocated to Fruit & Vegetables, but very little to the Dairy & Meat product areas.  It could be hypothesised that these customers are following a vegan diet.  Finally customers in *Cluster 2* spend, on average, significant portions within Dairy, Fruit & Vegetables, but very little in the Meat product area - so similarly, we would make an early hypothesis that these customers are more along the lines of those following a vegetarian diet - very interesting!


<br>
# Application <a name="kmeans-application"></a>

Even those this is a simple solution, based upon high level product areas it will help leaders in the business, and category managers gain a clearer understanding of the customer base.

Tracking these clusters over time would allow the client to more quickly react to dietary trends, and adjust their messaging and inventory accordingly.

Based upon these clusters, the client will be able to target customers more accurately - promoting products & discounts to customers that are truly relevant to them - overall enabling a more customer focused communication strategy.

<br>
# Growth & Next Steps <a name="growth-next-steps"></a>

It would be interesting to run this clustering/segmentation at a lower level of product areas, so rather than just the four areas of Meat, Dairy, Fruit, Vegetables - clustering spend across the sub-categories *below* those categories.  This would mean we could create more specific clusters, and get an even more granular understanding of dietary preferences within the customer base.

Here we've just focused on variables that are linked directly to sales - it could be interesting to also include customer metrics such as distance to store, gender etc to give a even more well-rounded customer segmentation.

It would be useful to test other clustering approaches such as hierarchical clustering or DBSCAN to compare the results.
